

GPT 的 T = transformer
- Attention is all you need - Google Research
- Gemini Encoder 方式不同於hugging face opensource llm
- Google Brain 不斷提倡 Attention 方式的改進
- Cross attention 和 self-attention 是自然語言處理中常用的兩種注意力機制。
- Google 2024/04 推出革命性的Infini-attention

Google發布MediaPipe LLM Inference API
- 開發者可以完全在"行動裝置"上執行大型語言模型
- 重要性在於讓大型語言模型能夠在裝置上運作，而且達到跨平臺相容
- 中文訓練資料只占0.04%, 提中簡體90%, 繁體10%

Mistral-TMMLUplus
- 1個 H100/A100 GPU
- Infini-attention

繁體中文模型
「防止中國AI文化侵略」
台灣第一個繁體中文大語言模型TAIDE (2024-01-22)
- 1028顆V100 GPU
- 9顆的H100 GPU

- ChatGPT4 - 180B
- Gemini - 70B
- TAIDE 基於Meta AI Llama 2 - 8B

TAIDE容易誤會指令意思，需要更精確周延的下達命令，背後的智能明顯不足。
台灣數位發展部 人工智慧評測中心正式掛牌，TAIDE即是第一個接受檢查的大語言模型（LLM）。

- TAIDE可以被調教成專業工具

可以將TAIDE理解成一個會說繁體中文的初步模型，各單位將這個模型帶回去後，再根據需求調教成擅長特定領域的工具。

聯發科技股份有限公司（TWSE：2454
- 開源語言模型 BLOOM
- 自製訓練平台 達哥
(繁體中文理解力5.9分平測低下, 達哥發表但未公開(但競業不可能使用))

InfuseAI
- 緯創領頭1.4億
- 從Mlops 走向 PrimeLM 但效率低下
# https://github.com/InfuseAI/PrimeLM

台灣製造業的隱私 邊緣運算 地端化