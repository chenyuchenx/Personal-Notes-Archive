

## Transformer
- Attention is All you Need
- Encoder
    - self-attention -> feed forward
    - Q1,K1,V1
- Decoder
    - self-attention -> attention -> feed forward
    - 
    - 
![alt text](image.png)